<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>The Pycnic</title><link href="http://blog.pycnic.org/" rel="alternate"></link><link href="http://blog.pycnic.org/feeds/all.atom.xml" rel="self"></link><id>http://blog.pycnic.org/</id><updated>2017-03-20T00:00:00+01:00</updated><entry><title>Setting up a DRMAA-compatible HPC cluster for reproducible research</title><link href="http://blog.pycnic.org/setting-up-a-drmaa-compatible-hpc-cluster-for-reproducible-research.html" rel="alternate"></link><published>2017-03-20T00:00:00+01:00</published><updated>2017-03-20T00:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-03-20:/setting-up-a-drmaa-compatible-hpc-cluster-for-reproducible-research.html</id><summary type="html">&lt;p&gt;This post is about setting up a HPC cluster that runs computational tasks with
a scheduler conforming to the
&lt;a href="https://en.wikipedia.org/wiki/DRMAA"&gt;Distributed Resource Management Application API (DRMAA)&lt;/a&gt; and has a
shared filesystem (NFS) of all master and slave nodes.
This includes Grid Engine (SGE).
Out of all available Python high-level abstraction frameworks …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is about setting up a HPC cluster that runs computational tasks with
a scheduler conforming to the
&lt;a href="https://en.wikipedia.org/wiki/DRMAA"&gt;Distributed Resource Management Application API (DRMAA)&lt;/a&gt; and has a
shared filesystem (NFS) of all master and slave nodes.
This includes Grid Engine (SGE).
Out of all available Python high-level abstraction frameworks for
computational task scheduling, we choose &lt;a href="https://distributed.readthedocs.io/en/latest/"&gt;Dask.distributed&lt;/a&gt;.
Note that &lt;a href="https://distributed.readthedocs.io/en/latest/setup.html"&gt;Dask.distributed is not restricted to DRMAA-compatible
clusters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In order to deploy Dask on DRMAA clusters, we choose &lt;a href="https://github.com/dask/dask-drmaa"&gt;dask-drmaa&lt;/a&gt;.
A prerequisite is the &lt;a href="https://github.com/pygridtools/drmaa-python"&gt;python-drmaa&lt;/a&gt; package which interfaces
to the cluster DRMAA library.
To &lt;a href="https://github.com/pygridtools/drmaa-python#installation"&gt;setup python-drmaa&lt;/a&gt;, one needs to make sure that the
cluster queueing settings are sourced (environment variables &lt;code&gt;SGE_ROOT&lt;/code&gt; and
&lt;code&gt;SGE_CELL&lt;/code&gt;), as well as that the environment variable &lt;code&gt;DRMAA_LIBRARY_PATH&lt;/code&gt;
is set to the &lt;code&gt;libdrmaa.so&lt;/code&gt; library of the system.&lt;/p&gt;
&lt;p&gt;For example, the &lt;a href="https://github.com/andsor/dotphiles/blob/62bdafd5ed46ab2657c8e1be9c04779d1798eb06/bashrc.d/login01#L6"&gt;settings on the MPIDS NLD clusters&lt;/a&gt; are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; /core/ge/NLD/common/settings.sh
&lt;span class="nv"&gt;DRMAA_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/core/ge/lib/linux-x64/libdrmaa.so
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DRMAA_LIBRARY_PATH&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$DRMAA_LIBRARY_PATH&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Furthermore, and for the time being, we install a &lt;a href="https://github.com/andsor/dask-drmaa"&gt;forked version of
dask-drmaa&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pip install git+https://github.com/andsor/dask-drmaa.git#egg&lt;span class="o"&gt;=&lt;/span&gt;dask-drmaa
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(&lt;strong&gt;Note to myself: Create pull requests to the &lt;a href="https://github.com/dask/dask-drmaa"&gt;original
dask-drmaa&lt;/a&gt;&lt;/strong&gt;.)&lt;/p&gt;
&lt;p&gt;An important abstraction is to &lt;strong&gt;separate computation from the setup of the
computational environment&lt;/strong&gt;.
This means that the computational code connects to a Dask Client, but is not
responsible for spinning up the cluster.
The only parameter the computational code should take is the &lt;a href="http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client"&gt;&lt;code&gt;address&lt;/code&gt;
parameter to connect to the scheduler&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# address : set either interactively or feed in as command-line argument&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dask.distributed&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;
&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Yes, this involves one manual step in both interactive and scripted computation:
actually spinning up the dask distributed cluster and manually feeding the
&lt;code&gt;address&lt;/code&gt; parameter into the notebook or as a command-line argument to the
script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Both interactive and scripted computation can opt for fixed-sized clusters or
adaptive clusters.
Adaptive clusters spawn additional workers upon demand (and stop them again).
Fixed-size clusters have a fixed number of workers up and running, even though
the actual computation is long finished.
Adaptive clusters are particularly efficient if the cluster is idle.
Basically, they scale to the cluster size if necessary but keep the cluster idle
otherwise.
When the cluster is under high load and busy, it takes some time to spawn the
first workers, and in the meantime a lot more other jobs might get submitted,
further increasing the time to spawning the next workers.
Hence, under high load a fixed-sized cluster might be more efficient, as it
reserves computational resources at once.
These considerations hold for interactive computation as well as for scripted
computation.&lt;/p&gt;
&lt;p&gt;The abstraction introduced above covers these cases, as the actual computation
only needs the &lt;code&gt;address&lt;/code&gt; parameter and does not care about the details of the
dask distributed cluster (results will stay the same, even though the runtime of
the computation will vary).&lt;/p&gt;
&lt;p&gt;To spin up the dask distributed cluster with dask-drmaa, either use the
command line or a Python snippet.
On the command line of the login or master node of the cluster, run&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ dask-drmaa -n &lt;span class="m"&gt;10&lt;/span&gt; -q queue.q
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;for a fixed-size cluster with 10 workers running on the queue &lt;code&gt;queue.q&lt;/code&gt;.
(NB: In the &lt;a href="https://github.com/andsor/dask-drmaa"&gt;dask-drmaa fork&lt;/a&gt;, workers consume only one
core.)
For an adaptive cluster, simply omit the number of workers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ dask-drmaa -q queue.q
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In both cases, the command will output a JSON string of the &lt;code&gt;address&lt;/code&gt;
parameter to &lt;code&gt;stdout&lt;/code&gt; for further manual or scripted processing.&lt;/p&gt;
&lt;p&gt;Now, we assume we have only SSH access to the cluster master or login node, and
cannot reach the Dask scheduler directly.
Luckily, we can "forward" the port the Dask scheduler listens to on the cluster
master or login node to our local host.
In order to forward the dask scheduler port to our local host, we use SSH
portforwarding:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ ssh -N -L &lt;span class="o"&gt;{&lt;/span&gt;port&lt;span class="o"&gt;}&lt;/span&gt;:&lt;span class="o"&gt;{&lt;/span&gt;hostip&lt;span class="o"&gt;}&lt;/span&gt;:&lt;span class="o"&gt;{&lt;/span&gt;port&lt;span class="o"&gt;}&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt;loginnode&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here, substitute the scheduler &lt;code&gt;address&lt;/code&gt; information and the SSH hostname for
the cluster master or login node.
Now, we connect to the dask scheduler on our local host with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# address is localhost:port&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dask.distributed&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;
&lt;span class="n"&gt;client&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;address&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The biggest caveat is that Bokeh is not forwarded this way.&lt;/p&gt;
&lt;p&gt;Further reading and examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://matthewrocklin.com/blog/work/2017/01/24/dask-custom"&gt;Custom Parallel Algorithms on a Cluster with Dask&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="reproducible research"></category><category term="python"></category><category term="conda"></category><category term="hpc"></category><category term="dask"></category><category term="sge"></category><category term="grid engine"></category><category term="mpids nld clusters"></category><category term="interactive computation"></category><category term="scripted computation"></category></entry><entry><title>Deploying Python scripts and packages internally</title><link href="http://blog.pycnic.org/deploying-python-scripts-and-packages-internally.html" rel="alternate"></link><published>2017-03-13T17:00:00+01:00</published><updated>2017-03-13T17:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-03-13:/deploying-python-scripts-and-packages-internally.html</id><summary type="html">&lt;p&gt;In developing Python scripts and packages for computational research, we
distinguish two different routes to openness.
The gold standard is you develop your code in the open right away.
This means creating a public repository on a public version-control repository
service like GitHub and regularly pushing all commits there.
The …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In developing Python scripts and packages for computational research, we
distinguish two different routes to openness.
The gold standard is you develop your code in the open right away.
This means creating a public repository on a public version-control repository
service like GitHub and regularly pushing all commits there.
The green standard is to develop the code internally first and upload it later
to a public repository, before the research paper becomes available online.&lt;/p&gt;
&lt;p&gt;On the golden route, continuous integration and deployment of scripts
and packages e.g. to PyPI is fine as the source is public anyway.
However, on the green route, the source is to remain internal for the time being
and hence, publishing the package to PyPI is not an option.
The question is, how to make sure that the developed code is well-integrated
into the Python ecosystem anyway.&lt;/p&gt;
&lt;p&gt;Here, we restrict ourselves to the following setting:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We do not need to build conda packages, pip packages will do.
  (We should build conda packages though as soon as we publish the source.)&lt;/li&gt;
&lt;li&gt;We use a private repository onto which we have command-line access
  (e.g. GitLab or GitHub with SSH key)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means, developing a Python package, and tagging versions along, we can
install a specific version, branch, or even commit, by using
&lt;a href="https://pip.pypa.io/en/latest/reference/pip_install/#git"&gt;pip with git&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install git:+ssh://SERVER/REPOSITORY-PATH&lt;span class="o"&gt;[&lt;/span&gt;@BRANCH&lt;span class="p"&gt;|&lt;/span&gt;VERSION&lt;span class="p"&gt;|&lt;/span&gt;COMMIT-HASH&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When installing from a remote machine, e.g. a HPC cluster, one needs to register
one's SSH key from that machine to the web-based version control repository
(GitLab / GitHub).&lt;/p&gt;</content><category term="reproducible research"></category><category term="python"></category><category term="gold route"></category><category term="green route"></category><category term="pip"></category><category term="conda"></category><category term="pypi"></category><category term="web-based version control repository"></category><category term="git"></category><category term="hpc"></category></entry><entry><title>Ingredients for a recipe for reproducible research</title><link href="http://blog.pycnic.org/ingredients-for-a-recipe-for-reproducible-research.html" rel="alternate"></link><published>2017-02-23T18:00:00+01:00</published><updated>2017-02-23T18:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-02-23:/ingredients-for-a-recipe-for-reproducible-research.html</id><summary type="html">&lt;p&gt;Doing computational research in a reproducible manner requires some effort in
the first place.
The good news is, project organisation and collaboration come almost for free.&lt;/p&gt;
&lt;p&gt;This series provides some guidance in setting up an individual or collaborative
computational research project that leads to a specific scientific output.
Project and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Doing computational research in a reproducible manner requires some effort in
the first place.
The good news is, project organisation and collaboration come almost for free.&lt;/p&gt;
&lt;p&gt;This series provides some guidance in setting up an individual or collaborative
computational research project that leads to a specific scientific output.
Project and output does not only refer to a paper, but also to other forms of
scientific output, such as presentations, a software package, or a dataset.&lt;/p&gt;
&lt;p&gt;As in cooking, there is a plethora of possible ingredients, and a lot of
feasible choices to combine them in setting up and running a computational
research project.
This recipe makes a lot of default choices and hence, provides a working
solution.
As in cooking, the chef is free to make her own choices to refine the recipe
or adapt it to personal taste.&lt;/p&gt;
&lt;p&gt;Specifically, the recipe employs the following ingredients:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Operating system:
  &lt;a href="https://en.wikipedia.org/wiki/Ubuntu_(operating_system)"&gt;&lt;i class="fa fa-linux" aria-hidden="true"&gt;&lt;/i&gt; Ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Command processor:
  &lt;a href="https://en.wikipedia.org/wiki/Bash_%28Unix_shell%29"&gt;&lt;i class="fa fa-terminal" aria-hidden="true"&gt;&lt;/i&gt;
  Bash&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;System configuration:
  dotphiles, https://dotfiles.github.io/&lt;/li&gt;
&lt;li&gt;Text editor:
  &lt;a href="https://atom.io/"&gt;Atom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Version control system:
  &lt;a href="https://en.wikipedia.org/wiki/Git"&gt;&lt;i class="fa fa-git" aria-hidden="true"&gt;&lt;/i&gt; Git&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Web-based version control repository:
  &lt;a href="https://about.gitlab.com"&gt;&lt;i class="fa fa-gitlab" aria-hidden="true"&gt;&lt;/i&gt; GitLab&lt;/a&gt;,
  &lt;a href="https://github.com/"&gt;&lt;i class="fa fa-github" aria-hidden="true"&gt;&lt;/i&gt; GitHub&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Commit message:
  https://docs.scipy.org/doc/numpy/dev/gitwash/development_workflow.html#writing-the-commit-message&lt;/li&gt;
&lt;li&gt;Scripting language:
  &lt;a href="https://www.python.org/"&gt;&lt;i class="fa fa-python" aria-hidden="true"&gt;&lt;/i&gt; Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Unit testing framework:
  &lt;a href="http://pytest.org"&gt;pytest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Property-based testing, including stateful testing:
  &lt;a href="http://hypothesis.works/"&gt;Hypothesis for Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Automate testing:
  &lt;a href="https://tox.readthedocs.io"&gt;tox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Versioning scheme:
  &lt;a href="http://semver.org/"&gt;Semantic Versioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Text source markup language:
  &lt;a href="https://en.wikipedia.org/wiki/Markdown"&gt;Markdown&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/ReStructuredText"&gt;reStructuredText&lt;/a&gt;, &lt;a href="https://www.latex-project.org/"&gt;LaTeX&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Text document converter:
  &lt;a href="https://en.wikipedia.org/wiki/Pandoc"&gt;Pandoc&lt;/a&gt;,
  &lt;a href="http://www.sphinx-doc.org/"&gt;Sphinx&lt;/a&gt;,
  &lt;a href="https://en.wikipedia.org/wiki/TeX_Live"&gt;TeX Live&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Virtualization software:
  &lt;a href="https://en.wikipedia.org/wiki/Docker_%28software%29"&gt;Docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reference management software:
  &lt;a href="https://www.zotero.org/"&gt;Zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Task management and automation:
  &lt;a href="http://pydoit.org/"&gt;doit&lt;/a&gt;,
  &lt;a href="http://ctan.org/pkg/latexmk"&gt;latexmk&lt;/a&gt;,
  &lt;a href="https://en.wikipedia.org/wiki/Make_(software)"&gt;Make&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Package and environment management:
  &lt;a href="https://conda.io"&gt;Conda&lt;/a&gt;, &lt;a href="https://pip.pypa.io"&gt;pip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;License:
  &lt;a href="https://creativecommons.org/licenses/by/4.0/"&gt;&lt;i class="fa fa-creative-commons" aria-hidden="true"&gt;&lt;/i&gt;
  Creative Commons Attribution License&lt;/a&gt;,
  &lt;a href="https://en.wikipedia.org/wiki/ISC_license"&gt;ISC License&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Interactive data science and scientific computing:
  &lt;a href="http://jupyter.org"&gt;Project Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reproducible reporting and literate programming:
  &lt;a href="http://www.ctan.org/pkg/pythontex"&gt;PythonTeX&lt;/a&gt;,
  &lt;a href="http://jupyter.org"&gt;Project Jupyter&lt;/a&gt;,
  &lt;a href="https://pystitch.github.io/"&gt;stitch&lt;/a&gt;,
  &lt;a href="http://mpastell.com/pweave/"&gt;Pweave&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Plotting:
  &lt;a href="http://holoviews.org"&gt;HoloViews&lt;/a&gt;, &lt;a href="http://bokeh.pydata.org"&gt;Bokeh&lt;/a&gt;, &lt;a href="http://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hosting documentation:
  &lt;a href="https://readthedocs.org/"&gt;Read The Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Continuous Integration (CI):
  &lt;a href="https://about.gitlab.com/gitlab-ci/"&gt;GitLab CI&lt;/a&gt;, &lt;a href="https://travis-ci.org/"&gt;Travis CI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random number generation&lt;/li&gt;
&lt;li&gt;Profiling&lt;/li&gt;
&lt;li&gt;Style Guide:
  &lt;a href="https://www.python.org/dev/peps/pep-0008/"&gt;PEP 8&lt;/a&gt;,
  &lt;a href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt"&gt;numpydoc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;High-Performance Computing (HPC):
  Grid Engine (SGE),
  &lt;a href="http://dask.pydata.org"&gt;Dask&lt;/a&gt;,
  Running dask on IPython parallel,
  Configuring IPython parallel on Grid Engine cluster,
  SSH,
  tmux,
  Random Number Generation in parallel computing&lt;/li&gt;
&lt;li&gt;Interactive High-Performance Computing (IHPC):
  Running Jupyter on a cluster node,
  &lt;a href="https://ipyparallel.readthedocs.io/"&gt;IPython Parallel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Research data repository (with DOI):
  &lt;a href="https://zenodo.org/"&gt;Zenodo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Collaborative writing and preprint publishing (with DOI):
  &lt;a href="https://www.authorea.com/"&gt;Authorea&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Persistent researcher identifier:
  &lt;a href="https://orcid.org/"&gt;ORCID&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Software repository:
  &lt;a href="https://anaconda.org"&gt;Anaconda Cloud&lt;/a&gt;,
  &lt;a href="https://pypi.python.org/pypi"&gt;Python Package Index&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Concepts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Artifact&lt;/li&gt;
&lt;li&gt;Profiling&lt;/li&gt;
&lt;li&gt;Random number generation&lt;/li&gt;
&lt;li&gt;Parallel Computing&lt;/li&gt;
&lt;li&gt;Task scheduling&lt;/li&gt;
&lt;li&gt;Task automation&lt;/li&gt;
&lt;li&gt;"Full" reproducibility&lt;/li&gt;
&lt;li&gt;Researcher identifier&lt;/li&gt;
&lt;li&gt;Research output repository&lt;/li&gt;
&lt;li&gt;Digital object identifier (DOI)&lt;/li&gt;
&lt;li&gt;Continuous integration (CI)&lt;/li&gt;
&lt;li&gt;Reproducible reporting and literate programming&lt;/li&gt;
&lt;li&gt;Unit testing&lt;/li&gt;
&lt;li&gt;Property-based testing&lt;/li&gt;
&lt;li&gt;Stateful testing&lt;/li&gt;
&lt;li&gt;Pinning dependencies&lt;/li&gt;
&lt;li&gt;Version control&lt;/li&gt;
&lt;li&gt;Workflow&lt;/li&gt;
&lt;li&gt;Release&lt;/li&gt;
&lt;li&gt;Semantic versioning&lt;/li&gt;
&lt;li&gt;Style convention&lt;/li&gt;
&lt;li&gt;Pull/Merge request&lt;/li&gt;
&lt;li&gt;Environment&lt;/li&gt;
&lt;li&gt;Issue&lt;/li&gt;
&lt;li&gt;Interactive computing&lt;/li&gt;
&lt;li&gt;Refactoring code into a library&lt;/li&gt;
&lt;li&gt;Reference management&lt;/li&gt;
&lt;li&gt;Long computations&lt;/li&gt;
&lt;li&gt;Build pipeline&lt;/li&gt;
&lt;li&gt;Storing large datasets&lt;/li&gt;
&lt;li&gt;Processing large datasets (out-of-core computation)&lt;/li&gt;
&lt;li&gt;Licensing&lt;/li&gt;
&lt;li&gt;Readme file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the heart of the recipe is the notion of an &lt;em&gt;artifact&lt;/em&gt;, as inspired by
&lt;a href="http://www.bekolay.org/scipy2013-workflow/#/"&gt;Bekolay (2013)&lt;/a&gt;.
Note that not every ingredient applies to every type of artifact.
The following posts in this series will shed light on each ingredient as well
as on several types of artifacts, such as publication-quality research papers,
research reports, software packages and presentations.&lt;/p&gt;</content><category term="reproducible research"></category></entry><entry><title>Introducing the "Doing Science" category</title><link href="http://blog.pycnic.org/introducing-the-doing-science-category.html" rel="alternate"></link><published>2017-02-23T15:00:00+01:00</published><updated>2017-02-23T15:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-02-23:/introducing-the-doing-science-category.html</id><summary type="html">&lt;p&gt;The new &lt;a href="/category/doing-science.html"&gt;&lt;strong&gt;Doing Science&lt;/strong&gt; category&lt;/a&gt; will hosts
posts aimed at the individual scientist and group.
The posts will be dealing with how to organise individual and collaborative
research and other scientific work on the work level.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The new &lt;a href="/category/doing-science.html"&gt;&lt;strong&gt;Doing Science&lt;/strong&gt; category&lt;/a&gt; will hosts
posts aimed at the individual scientist and group.
The posts will be dealing with how to organise individual and collaborative
research and other scientific work on the work level.&lt;/p&gt;</content><category term="about"></category></entry><entry><title>Using categories and tags</title><link href="http://blog.pycnic.org/using-categories-and-tags.html" rel="alternate"></link><published>2017-02-23T14:00:00+01:00</published><updated>2017-02-23T14:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-02-23:/using-categories-and-tags.html</id><summary type="html">&lt;p&gt;Digital assets abound and come in diverse flavors.
For example, a digital asset can be a blog post, a scientific article, a
software package.
Specific repositories store digital assets of the same type.
A blog stores blog posts, a scientific journal stores scientific articles, a
software package repository stores you …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Digital assets abound and come in diverse flavors.
For example, a digital asset can be a blog post, a scientific article, a
software package.
Specific repositories store digital assets of the same type.
A blog stores blog posts, a scientific journal stores scientific articles, a
software package repository stores you name it.
With storing a great number of digital assets comes the task of describing the
content of each asset and the relation of assets in a systematic way.
This meta-information facilitates retrieval of the desired asset as well as
exploration of the repository.&lt;/p&gt;
&lt;p&gt;Many systems in general and blogging systems in particular provide &lt;em&gt;categories&lt;/em&gt;
and &lt;a href="https://en.wikipedia.org/wiki/Tag_(metadata)#Knowledge_tags"&gt;&lt;em&gt;tags&lt;/em&gt;&lt;/a&gt; to
classify their contents.
Categories serve as thematically broad entry points and navigation to the
reader.
A category page is a distinct sub-blog, like a
newspaper section.
Tags on the other hand classify the ingredients of each piece across categories,
each post should have as many tags as relevant to its content.&lt;/p&gt;
&lt;p&gt;To summarize, &lt;em&gt;categories bind the content to a specific outlet (output), tags
classify the content itself (or the input)&lt;/em&gt;.&lt;/p&gt;</content><category term="blogging"></category><category term="digital asset management"></category></entry><entry><title>Introducing the Meta category</title><link href="http://blog.pycnic.org/introducing-the-meta-category.html" rel="alternate"></link><published>2017-02-23T00:00:00+01:00</published><updated>2017-02-23T00:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-02-23:/introducing-the-meta-category.html</id><summary type="html">&lt;p&gt;The new &lt;a href="/category/meta.html"&gt;&lt;strong&gt;Meta&lt;/strong&gt; category&lt;/a&gt; will be a home to all
blog posts concerned with running this blog.&lt;/p&gt;</summary><content type="html">&lt;p&gt;The new &lt;a href="/category/meta.html"&gt;&lt;strong&gt;Meta&lt;/strong&gt; category&lt;/a&gt; will be a home to all
blog posts concerned with running this blog.&lt;/p&gt;</content><category term="about"></category></entry><entry><title>Hello World!</title><link href="http://blog.pycnic.org/hello-world.html" rel="alternate"></link><published>2017-02-10T14:00:00+01:00</published><updated>2017-02-10T14:00:00+01:00</updated><author><name>Andreas Sorge</name></author><id>tag:blog.pycnic.org,2017-02-10:/hello-world.html</id><summary type="html">&lt;p&gt;Welcome to "The Pycnic", the blog of the pycnic team.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Hello World!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Welcome to "The Pycnic", the blog of the pycnic team.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Hello World!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="pycnic"></category><category term="about"></category></entry></feed>